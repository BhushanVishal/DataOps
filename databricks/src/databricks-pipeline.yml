trigger:
- master

pool:
  vmImage: 'ubuntu-latest'

stages:
  - stage: build
    jobs:
      - job:
        steps:

        - task: CopyFiles@2
          inputs:
            SourceFolder: '$(Build.SourcesDirectory)/databricks/src'
            Contents: '**'
            TargetFolder: '$(Build.ArtifactsStagingDirectory)/src'

        - task: PublishPipelineArtifact@1
          inputs:
            targetPath: '$(Build.ArtifactsStagingDirectory)'
            artifact: 'databricks'
            publishLocation: 'pipeline'

  - stage: 
    jobs:
      - deployment: 
        environment: dev
        strategy:
          runOnce:
            deploy:
              steps:
              - task: UsePythonVersion@0
                displayName: Configuring Python Version
                inputs:
                  versionSpec: '3.x'
                  addToPath: true
                  architecture: 'x64'

              - powershell: |
                  Write-Output "[AZDO]" | Out-File ~/.databrickscfg -Encoding ASCII
                  Write-Output "host = $(databricks.host)" | Out-File ~/.databrickscfg -Encoding ASCII -Append
                  Write-Output "token = $(databricks.token)" | Out-File ~/.databrickscfg -Encoding ASCII -Append

                  pip3 install --upgrade databricks-cli
                displayName: configuring databricks

              - powershell: |
                  databricks workspace list 
                  databricks workspace import_dir -o --profile AZDO $(Pipeline.Workspace)/databricks/src/ /src
                displayName: deploying notebooks
